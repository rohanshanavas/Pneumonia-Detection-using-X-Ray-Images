{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {},
        "colab_type": "code",
        "id": "htSU2KhD5_NL",
        "outputId": "e3c88a80-b003-438a-f43d-19a39978004f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import gc\n",
        "import imgaug as aug\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import imgaug.augmenters as iaa\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from keras.models import Sequential, Model,load_model\n",
        "from keras.applications.vgg16 import VGG16,preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import cv2\n",
        "from skimage.segmentation import slic\n",
        "from keras import backend as K\n",
        "\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jyZLd7pg5_NU"
      },
      "outputs": [],
      "source": [
        "data_dir = Path(\"data/chest_xray/chest_xray\")\n",
        "train_dir= data_dir/'train'\n",
        "val_dir=data_dir/'val'\n",
        "test_dir = data_dir / 'test'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JALzR-VI5_NY"
      },
      "source": [
        "A basic function to load train images from the directory and save it in a dataframe with labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LNukbkcM5_NZ"
      },
      "outputs": [],
      "source": [
        "def load_train():\n",
        "    normal_cases_dir = train_dir / 'NORMAL'\n",
        "    pneumonia_cases_dir = train_dir / 'PNEUMONIA'\n",
        "\n",
        "    # Get the list of all the images\n",
        "    normal_cases = normal_cases_dir.glob('*.jpeg')\n",
        "    pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
        "    train_data=[]\n",
        "    train_label=[]\n",
        "    for img in normal_cases:\n",
        "            train_data.append(img)\n",
        "            train_label.append(0)\n",
        "    for img in pneumonia_cases:\n",
        "        train_data.append(img)\n",
        "        train_label.append(1)\n",
        "    df=pd.DataFrame(train_data)\n",
        "    df.columns=['images']\n",
        "    df['labels']=train_label\n",
        "    df=df.sample(frac=1).reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6RjlLS1w5_Ne",
        "outputId": "e4e76c56-940e-4353-d1e3-8cf85a71f0b7"
      },
      "outputs": [],
      "source": [
        "train_data=load_train()\n",
        "len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EuFYnbgZ5_Nm"
      },
      "source": [
        "### Loading and preprocessing validation and test data\n",
        "Steps:-\n",
        "* Load the image using imread()\n",
        "* Since the images are of different length and widths, resize them to 224,224.\n",
        "* Some images in our data are greyscale (1 channel) , therefore convert them to 3 channel\n",
        "* Images using cv2 are read in BGR format(by default) , convert it to RGB.\n",
        "* Normalize the image pixels by dividing by 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fzP9Mbnn5_Nn"
      },
      "outputs": [],
      "source": [
        "def prepare_and_load(isval=True):\n",
        "    if isval==True:\n",
        "        normal_dir=val_dir/'NORMAL'\n",
        "        pneumonia_dir=val_dir/'PNEUMONIA'\n",
        "    else:\n",
        "        normal_dir=test_dir/'NORMAL'\n",
        "        pneumonia_dir=test_dir/'PNEUMONIA'\n",
        "    normal_cases = normal_dir.glob('*.jpeg')\n",
        "    pneumonia_cases = pneumonia_dir.glob('*.jpeg')\n",
        "    data,labels=([] for x in range(2))\n",
        "    def prepare(case):\n",
        "        for img in case:\n",
        "            img = cv2.imread(str(img))\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            if img.shape[2] ==1:\n",
        "                 img = np.dstack([img, img, img])\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = img.astype(np.float32)/255.\n",
        "            if case==normal_cases:\n",
        "                label = to_categorical(0, num_classes=2)\n",
        "            else:\n",
        "                label = to_categorical(1, num_classes=2)\n",
        "            data.append(img)\n",
        "            labels.append(label)\n",
        "        return data,labels\n",
        "    prepare(normal_cases)\n",
        "    d,l=prepare(pneumonia_cases)\n",
        "    d=np.array(d)\n",
        "    l=np.array(l)\n",
        "    return d,l\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "trDsG_7w5_Np"
      },
      "outputs": [],
      "source": [
        "val_data,val_labels=prepare_and_load(isval=True)\n",
        "test_data,test_labels=prepare_and_load(isval=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAG69Itl5_Ns"
      },
      "source": [
        "### Generating batches of images for training\n",
        "* It involves the same preprocessing steps as above, except that images are processed and returned in batches,defined by the batch size.\n",
        "* We also use Image segmentation ( the slic function of skimage).Image segmentation is the process of partitioning a  image into multiple segments (sets of pixels, also known as super-pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.\n",
        "* The dataset we have is Imbalanced and has pneumonia cases three times the normal cases.The goal of our model is to optimize accuracy while training , which it can easily do by classifying most of the cases as infected(since the majority cases are infected, the model will have high accuracy), but it is biased aginst the underrepresented class.Thus we try to augment images of the normal class , by adding flipped , rotated and changing brightness of the original images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yPKzfpB85_Nt"
      },
      "outputs": [],
      "source": [
        "def data_gen(data, batch_size):\n",
        "    # Get total number of samples in the data\n",
        "    n = len(data)\n",
        "    steps = n//batch_size\n",
        "    \n",
        "    # Define two numpy arrays for containing batch data and labels\n",
        "    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
        "    batch_labels = np.zeros((batch_size,2), dtype=np.float32)\n",
        "\n",
        "    # Get a numpy array of all the indices of the input data\n",
        "    indices = np.arange(n)\n",
        "    \n",
        "    # Initialize a counter\n",
        "    i =0\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        # Get the next batch \n",
        "        count = 0\n",
        "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
        "        for j, idx in enumerate(next_batch):\n",
        "            img_name = data.iloc[idx]['images']\n",
        "            label = data.iloc[idx]['labels']\n",
        "            \n",
        "            # one hot encoding\n",
        "            encoded_label = to_categorical(label, num_classes=2)\n",
        "            # read the image and resize\n",
        "            img = cv2.imread(str(img_name))\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            \n",
        "            # check if it's grayscale\n",
        "            if img.shape[2]==1:\n",
        "                img = np.dstack([img, img, img])\n",
        "            \n",
        "            # cv2 reads in BGR mode by default\n",
        "            orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            # normalize the image pixels\n",
        "            orig_img = img.astype(np.float32)/255.\n",
        "            \n",
        "            #segmentation\n",
        "            oig_img=slic(orig_img) \n",
        "            \n",
        "            batch_data[count] = orig_img\n",
        "            batch_labels[count] = encoded_label\n",
        "            #augmentation\n",
        "            seq = iaa.OneOf([\n",
        "                 iaa.Fliplr(), # horizontal flips\n",
        "                 iaa.Affine(rotate=20), # roatation\n",
        "                 iaa.Multiply((1.2, 1.5))]) #random brightness\n",
        "            # generating more samples of the undersampled class\n",
        "            if label==0 and count < batch_size-2:\n",
        "                aug_img1 = seq.augment_image(img)\n",
        "                aug_img2 = seq.augment_image(img)\n",
        "                aug_img1 = cv2.cvtColor(aug_img1, cv2.COLOR_BGR2RGB)\n",
        "                aug_img2 = cv2.cvtColor(aug_img2, cv2.COLOR_BGR2RGB)\n",
        "                aug_img1 = aug_img1.astype(np.float32)/255.\n",
        "                aug_img2 = aug_img2.astype(np.float32)/255.\n",
        "\n",
        "                batch_data[count+1] = aug_img1\n",
        "                batch_labels[count+1] = encoded_label\n",
        "                batch_data[count+2] = aug_img2\n",
        "                batch_labels[count+2] = encoded_label\n",
        "                count +=2\n",
        "            \n",
        "            else:\n",
        "                count+=1\n",
        "            \n",
        "            if count==batch_size-1:\n",
        "                break\n",
        "            \n",
        "        i+=1\n",
        "        yield batch_data, batch_labels\n",
        "            \n",
        "        if i>=steps:\n",
        "            i=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zLKBHnVV5_Ny"
      },
      "source": [
        "# Fine Tuned Model\n",
        "To train a neural network from scratch,we need a lot of data also lot of processing power and time , which is obviously unavailble and impractical. Therefore, we fine-tune pretrained neural networks. There are a lot of neural networks pretrained on billions of images, that can be used by changing the top layer as per our data.But we use a better technique than that called fine-tuning in which we tweak the parameters of an already trained network so that it adapts to the new task at hand. \n",
        "\n",
        "The initial layers learn very general features and as we go higher up the network, the layers tend to learn patterns more specific to the task it is being trained on. Thus, for fine-tuning, we want to keep the initial layers intact ( or freeze them ) and retrain the later layers for our task.\n",
        "\n",
        "We have used the VGG16 model and added our own dense layers at the bottom, then we froze the network upto the second last convolutional block, after which we retrain.\n",
        "\n",
        "![](https://i.imgur.com/Jjh8f0z.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5CQIuPtK5_N0"
      },
      "outputs": [],
      "source": [
        "def vgg16_model( num_classes=None):\n",
        "\n",
        "    model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "    x=Dense(1024, activation='relu')(model.layers[-4].output)# add my own dense layer after the last conv block\n",
        "    x=Dropout(0.7)(x)\n",
        "    x=Dense(512,activation='relu')(x)\n",
        "    x=Dropout(0.5)(x)\n",
        "    x=Dense(2,activation='softmax')(x)\n",
        "    model=Model(model.input,x)\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KTLDb2ti5_N4",
        "outputId": "a5287270-94c7-4646-c0e0-340a99dc8781"
      },
      "outputs": [],
      "source": [
        "vgg_conv=vgg16_model(2)\n",
        "for layer in vgg_conv.layers[:-10]:#freeze all layers except the last ten\n",
        "    layer.trainable = False\n",
        " \n",
        "# Check the trainable status of the individual layers\n",
        "for layer in vgg_conv.layers:\n",
        "    print(layer, layer.trainable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "i3B0DOP85_N8",
        "outputId": "0e6e632a-50f4-440c-bfc5-35cebd25536d"
      },
      "outputs": [],
      "source": [
        "vgg_conv.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bnW_IqYz5_OB"
      },
      "outputs": [],
      "source": [
        "opt = Adam(lr=0.0001, decay=1e-5)\n",
        "early_stop = EarlyStopping(monitor='loss',patience=3,verbose=1)\n",
        "vgg_conv.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nxzsbHCW5_OE",
        "outputId": "fb9737ec-c26a-4396-bd9e-2541d107adbb"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "nb_epochs = 5\n",
        "\n",
        "# Get a train data generator\n",
        "train_data_gen = data_gen(data=train_data, batch_size=batch_size)\n",
        "\n",
        "# Define the number of training steps\n",
        "nb_train_steps = train_data.shape[0]//batch_size\n",
        "\n",
        "print(\"Number of training and validation steps: {} and {}\".format(nb_train_steps, len(val_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YxCFDc5Y5_OI",
        "outputId": "b1457f89-7ac1-4576-a318-986c69cd6597"
      },
      "outputs": [],
      "source": [
        "# # Fit the model\n",
        "history = vgg_conv.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                              validation_data=(val_data,val_labels),callbacks=[early_stop],\n",
        "                               class_weight={0:1.0, 1:0.4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "740AB4j95_OP"
      },
      "source": [
        "Let's see how it does on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I0HGuoCw5_OT",
        "outputId": "f7d4624e-8547-4990-b4d3-56c1512e4de2"
      },
      "outputs": [],
      "source": [
        "loss,acc=vgg_conv.evaluate(test_data,test_labels,batch_size=16)\n",
        "print('Loss and accuracy',loss,'&',acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M9V5gCYl5_Oa"
      },
      "source": [
        "Since we are working on Imbalanced data, accuracy is not really a trustworthy measure. Let's view the classification report first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XEW-EOV35_Oc",
        "outputId": "62bfdc80-f821-47d6-f725-fa7e9af6b799"
      },
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "pred = vgg_conv.predict(test_data, batch_size=16)\n",
        "pred = np.argmax(pred, axis=-1)\n",
        "\n",
        "# Original labels\n",
        "labels = np.argmax(test_labels, axis=-1)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(labels, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kzo7MwwM5_Ox"
      },
      "source": [
        "**Precision** is a fraction of people actually having pneumonia to all those predicted by the model as having pneumonia. **Recall/Sensitivity** on the other hand refers to the fraction of people actually having pneumonia and are predicted positive by the model to the total number of people having pneumonia. Hence, it relates to the potential of a test to recognise subjects with the disease.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AuNyR-1H5_Oy",
        "outputId": "e8a04d22-6dbd-46cf-cd4f-cb458e07f1d1"
      },
      "outputs": [],
      "source": [
        "# Get the confusion matrix\n",
        "cm  = confusion_matrix(labels, pred)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
        "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lijsZhHJ5_O3",
        "outputId": "ee075825-4fc3-4cf0-9b83-f1c1ba9a2abe"
      },
      "outputs": [],
      "source": [
        "# Calculate Precision and Recall\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "\n",
        "print(\"Recall/Sensitivity of the model is {:.2f}\".format(recall))\n",
        "print(\"Precision of the model is {:.2f}\".format(precision))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z3Lh7-5B5_O9"
      },
      "source": [
        "### AUC-ROC Curve\n",
        " It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. For our case, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. For an ideal model, the AUC is close to 1 and for a model as good as random guessing it's 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rZASyivr5_O9",
        "outputId": "cdcba9f2-08c5-4585-cc49-24529f36b268"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "fpr, tpr, threshold = metrics.roc_curve(labels, pred)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "# method I: plt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sDQQ1jFi5_PE"
      },
      "source": [
        "## Class Activation Maps\n",
        "Here, I have used Gradient-weighted Class Activation Maps.It uses the gradients of any target concept (say logits for ‘cat’), flowing into the\n",
        "final convolutional layer to produce a coarse localization\n",
        "map highlighting the important regions in the image for predicting the concept.\n",
        "\n",
        "So, to explain in simple terms, we simply take the final convolutional feature map and then we weigh every channel in that feature with the gradient of the class with respect to the channel. It’s just nothing but how intensely the input image activates different channels by how important each channel is with regard to the class. The best part is it doesn’t require any re-training or change in the existing architecture unlike CAM where a Global Average Pooling layer is needed to generate activations.\n",
        "\n",
        "![](https://pbs.twimg.com/media/DTRSmgHXcAEYask.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "N4U7hPkp5_PE"
      },
      "outputs": [],
      "source": [
        "\n",
        "from skimage import data, color, io, img_as_float\n",
        "def get_heatmap(processed_image, class_idx):\n",
        "    # we want the activations for the predicted label\n",
        "    class_output = vgg_conv.output[:, class_idx]\n",
        "    \n",
        "    # choose the last conv layer in your model\n",
        "    last_conv_layer = vgg_conv.get_layer('block5_conv3')\n",
        "    \n",
        "    # get the gradients wrt to the last conv layer\n",
        "    grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
        "    \n",
        "   # we pool the gradients over all the axes leaving out the channel dimension\n",
        "    pooled_grads = K.mean(grads, axis=(0,1,2))\n",
        "    \n",
        "    # Define a function that generates the values for the output and gradients\n",
        "    iterate = K.function([vgg_conv.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "    \n",
        "    # get the values\n",
        "    grads_values, conv_ouput_values = iterate([processed_image])\n",
        "    \n",
        "    # iterate over each feature map in your conv output and multiply\n",
        "    # the gradient values with the conv output values. This gives an \n",
        "    # indication of \"how important a feature is\"\n",
        "    for i in range(512): # we have 512 features in our last conv layer\n",
        "        conv_ouput_values[:,:,i] *= grads_values[i]\n",
        "    \n",
        "    # create a heatmap\n",
        "    heatmap = np.mean(conv_ouput_values, axis=-1)\n",
        "    \n",
        "    # remove negative values\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    \n",
        "    # normalize\n",
        "    heatmap /= heatmap.max()\n",
        "    \n",
        "    return heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XJq_zMCt5_PH",
        "outputId": "e1208b9f-8fa6-4df4-dfd6-ecde4164bc83"
      },
      "outputs": [],
      "source": [
        "\n",
        "# select the sample and read the corresponding image and label\n",
        "sample_image = cv2.imread('../input/chest_xray/chest_xray/val/PNEUMONIA/person1947_bacteria_4876.jpeg')\n",
        "# pre-process the image\n",
        "sample_image = cv2.resize(sample_image, (224,224))\n",
        "if sample_image.shape[2] ==1:\n",
        "            sample_image = np.dstack([sample_image, sample_image, sample_image])\n",
        "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
        "sample_image = sample_image.astype(np.float32)/255.\n",
        "sample_label = 1\n",
        "    \n",
        "    \n",
        "sample_image_processed = np.expand_dims(sample_image, axis=0)#since we pass only one image,we expand dim to include\n",
        "                                                             #batch size 1\n",
        "    \n",
        "# get the label predicted by our original model\n",
        "pred_label = np.argmax(vgg_conv.predict(sample_image_processed), axis=-1)[0]\n",
        "    \n",
        "    \n",
        "# get the heatmap for class activation map(CAM)\n",
        "heatmap = get_heatmap(sample_image_processed, pred_label)\n",
        "heatmap = cv2.resize(heatmap, (sample_image.shape[0], sample_image.shape[1]))\n",
        "heatmap = heatmap *255\n",
        "heatmap = np.clip(heatmap, 0, 255).astype(np.uint8)\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "#superimpose the heatmap on the image    \n",
        "\n",
        "sample_image_hsv = color.rgb2hsv(sample_image)\n",
        "heatmap = color.rgb2hsv(heatmap)\n",
        "\n",
        "alpha=0.7\n",
        "sample_image_hsv[..., 0] = heatmap[..., 0]\n",
        "sample_image_hsv[..., 1] = heatmap[..., 1] * alpha\n",
        "\n",
        "img_masked = color.hsv2rgb(sample_image_hsv)\n",
        "\n",
        "f,ax = plt.subplots(1,2, figsize=(16,6))\n",
        "ax[0].imshow(sample_image)\n",
        "ax[0].set_title(f\"True label: {sample_label} \\n Predicted label: {pred_label}\")\n",
        "ax[0].axis('off')\n",
        "    \n",
        "ax[1].imshow(img_masked)\n",
        "ax[1].set_title(\"Class Activation Map\")\n",
        "ax[1].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DizfeXrL5_PK"
      },
      "source": [
        "CAM proves to be useful in interpreting the model, to know whether the CNN percieves x-ray images as radiologists do, or whether it learns unuseful features to make predictions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BVLNYue57VMK"
      },
      "outputs": [],
      "source": [
        "vgg_conv.save('models/model.h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "pneumonia-detection-fine-tuning-and-cam (1).ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
